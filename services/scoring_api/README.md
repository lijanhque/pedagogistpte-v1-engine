# PTE Academic Scoring Engine (Production Backend)

A fully-featured **backend-only** scoring engine for PTE Academic exam responses. Deployable to **Vercel** or **Motia Cloud** with support for hybrid NLP + AI scoring, async job processing, workflow orchestration, and real-time streaming.

## ğŸ¯ Features

- âœ… **Level 1:** CRUD API + sync/async scoring endpoints
- âœ… **Level 2:** RQ background job queue with batch processing
- âœ… **Level 3:** Workflow orchestrator with Redis state management
- âœ… **Level 4:** Hybrid scoring (50% local NLP + 50% Vercel AI Gateway)
- âœ… **Level 5:** Real-time updates via Server-Sent Events (SSE)
- âœ… **Production-ready:** Docker, CI/CD, comprehensive tests, monitoring

## ğŸ“Š PTE Scoring Dimensions

Scores responses across four core dimensions (0-90 scale):

1. **Fluency** â€” Response length, word complexity, discourse structure, filler word detection
2. **Pronunciation** â€” Word syllable patterns, stress patterns, metadata hints
3. **Lexical Range** â€” Vocabulary diversity (Type-Token Ratio), lexical density, advanced vocabulary
4. **Grammar** â€” Sentence structure variety, punctuation consistency, clause complexity
5. **Overall** â€” Weighted average of above four

All scoring is **interpretable** (no black-box ML) and **instant** (no external API latency needed).

## ğŸš€ Quick Start

### Local Development

```bash
# Clone and install
git clone <repo>
cd pedagogistpte-v1-engine

# Set up environment
cp services/scoring_api/.env.example .env

# Build and run
docker compose up --build

# Test
curl -X POST http://localhost:8000/score \
  -H "Content-Type: application/json" \
  -d '{"text": "The rapid development of technology has changed our lives significantly."}'

# OpenAPI docs
open http://localhost:8000/docs
```

### Deploy to Vercel

```bash
git push origin main
# Vercel auto-deploys via GitHub webhook

# Check deployment
curl https://your-project.vercel.app/health
```

### Deploy to Motia Cloud

```bash
npm run build
motia deploy --service scoring_api
```

## ğŸ“ API Endpoints

### Scoring

**Sync scoring (returns immediately):**
```bash
POST /score
Content-Type: application/json

{
  "text": "Response text to score",
  "metadata": {"clarity_rating": 8}
}

# Returns
{
  "scores": {
    "fluency": 72,
    "pronunciation": 75,
    "lexical_range": 68,
    "grammar": 70,
    "overall": 71
  },
  "model": "pte_nlp_scorer"
}
```

**Async scoring (returns job_id):**
```bash
POST /score_async
Content-Type: application/json

{"text": "..."}

# Returns
{"job_id": "job-uuid", "status": "queued"}

# Poll for result
GET /job/job-uuid
```

### Batch Processing

```bash
POST /batch_score
Content-Type: application/json

[
  {"text": "Sample 1"},
  {"text": "Sample 2"}
]

# Returns
{"job_ids": ["job-1", "job-2"], "count": 2}
```

### Real-Time Streaming

```bash
# Create a workflow
POST /workflow/create
{"assessment_id": "ASSESS-001", "submission": {"text": "..."}}

# Returns
{"workflow_id": "workflow-uuid"}

# Stream updates
GET /workflow/workflow-uuid/stream
# EventSource receives: data: {"status": "...", "scores": {...}}\n\n
```

### CRUD Operations

```bash
# Create assessment
POST /assessments
{"student_id": "STU-001", "metadata": {...}}

# Get assessment
GET /assessments/{assessment_id}
```

### Monitoring

```bash
GET /health          # Service health & features
GET /metrics         # Queue size, Redis status
```

## ğŸ”‘ Environment Variables

```bash
# Required
REDIS_URL=redis://localhost:6379/0

# AI Gateway Keys (optional but recommended)
VERCEL_AI_GATEWAY_KEY=your_key_here
GOOGLE_GENAI_KEY=your_key_here

# Scoring mode
SCORE_MODE=sync  # or 'async'

# Optional
DATABASE_URL=postgresql://...  # For audit logs
APP_ENV=production
```

## ğŸ— Architecture

See [ARCHITECTURE.md](../../ARCHITECTURE.md) for detailed design.

```
FastAPI Server (Vercel/Motia)
    â†“
NLP Scorer (instant local scoring)
    â†“
Vercel AI Gateway (optional hybrid scoring)
    â†“
Redis Queue (background jobs)
    â†“
RQ Worker (async processing)
    â†“
SSE Publisher (real-time updates)
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest services/scoring_api/tests/ -v

# With coverage
pytest services/scoring_api/tests/ --cov=app --cov-report=html

# Specific test
pytest services/scoring_api/tests/test_comprehensive.py::TestNLPScorer -v
```

## ğŸ“¦ Production Deployment

See [DEPLOY_BACKEND.md](../../DEPLOY_BACKEND.md) for step-by-step instructions.

**Deployment checklist:**
- [ ] Redis configured and accessible
- [ ] Environment variables set (VERCEL_AI_GATEWAY_KEY, etc.)
- [ ] Tests passing
- [ ] Docker image builds successfully
- [ ] Monitoring/logging enabled

## ğŸ“Š Performance

- **Sync scoring latency:** ~5-10ms (NLP only) or ~200-500ms (with AI Gateway)
- **Async throughput:** Limited by queue worker count (can scale horizontally)
- **Memory:** ~100-150MB per instance

## ğŸ”’ Security

- Input validation via Pydantic
- API key management via environment variables
- Rate limiting (optional via slowapi middleware)
- Audit logging support (optional Postgres)
- TLS/HTTPS for all external API calls

## ğŸ›  Files Structure

```
services/scoring_api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main_v2.py              # Main API (all 5 levels)
â”‚   â”œâ”€â”€ worker_v2.py            # RQ worker for async jobs
â”‚   â”œâ”€â”€ schemas.py              # Pydantic models
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pte_nlp_scorer.py  # PTE scoring logic (fluency, etc.)
â”‚   â”‚   â””â”€â”€ workflow_orchestrator.py  # Workflow state management
â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â””â”€â”€ vercel_gateway.py   # AI Gateway integration
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â””â”€â”€ sse.py             # Server-Sent Events
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_comprehensive.py    # Full test suite
â”‚   â””â”€â”€ test_main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

## ğŸ¤ Contributing

1. Add feature in a new branch
2. Add tests in `tests/test_*.py`
3. Run `pytest` and linters (`black`, `flake8`)
4. Push and open a PR

## ğŸ“– Documentation

- [DEPLOY_BACKEND.md](../../DEPLOY_BACKEND.md) â€” Deployment guide
- [ARCHITECTURE.md](../../ARCHITECTURE.md) â€” Technical architecture
- OpenAPI docs: `http://localhost:8000/docs` (when running locally)

## ğŸ“ Support

Check logs in Vercel/Motia Cloud dashboard or run locally with `docker compose logs scoring_api`.
